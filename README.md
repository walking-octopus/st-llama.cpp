# LLaMA.cpp Lab ðŸ¦™ðŸ§ª

> A lovely 100-LOC script to chat with LLaMA.cpp.

## What?

It's a tiny 110 LOC StreamLit app to chat with LLaMA using `llama_cpp_python`.

## Why?

Other inference web-UIs seem to be huge projects with hundreds of dependenceies, especially the CUDA, and XInference doesn't support custom GGML models.

If you just want a simple tiny front-end for chatting with these models on a CPU, that's what it's here for.

## TODO:
- Streaming [x]
- Chat mode []
- Timings []
- Model presets []
- Downloads and GGML repository (some JSON file listing all the recent models, their short descriptions, and links to HF's direct download URLs) []